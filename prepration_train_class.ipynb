{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c0b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "import yaml\n",
    "\n",
    "class DataPreparation:\n",
    "    def __init__(self, config_file):\n",
    "        with open(config_file, 'r') as f:\n",
    "            try:\n",
    "                config = yaml.safe_load(f)\n",
    "                self.file_path = config['sensor_file']\n",
    "                self.sensor_data = pd.read_csv(self.file_path)\n",
    "            except yaml.YAMLError as exc:\n",
    "                print(exc)\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        # Drop unnecessary columns, if any\n",
    "        columns_to_drop = []  # Specify columns to drop, if necessary\n",
    "        self.sensor_data = self.sensor_data.drop(columns=columns_to_drop)\n",
    "        \n",
    "        # Ensure all remaining columns are numeric\n",
    "        self.sensor_data = self.sensor_data.apply(pd.to_numeric, errors='coerce')\n",
    "        \n",
    "        # Extract column names for future use\n",
    "        columns = self.sensor_data.columns\n",
    "        \n",
    "        # Convert to NumPy array for sklearn transformations\n",
    "        sensor_data_np = self.sensor_data.values\n",
    "        \n",
    "        # Impute missing values with median (for training data)\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        sensor_data_np = imputer.fit_transform(sensor_data_np)\n",
    "        \n",
    "        # Standardize the data (for training data)\n",
    "        scaler = StandardScaler()\n",
    "        sensor_data_np = scaler.fit_transform(sensor_data_np)\n",
    "        \n",
    "        # Check if columns length matches the number of columns in sensor_data_np\n",
    "        if sensor_data_np.shape[1] != len(columns):\n",
    "            raise ValueError(f\"Number of columns after preprocessing ({sensor_data_np.shape[1]}) does not match original number of columns ({len(columns)})\")\n",
    "        \n",
    "        # Convert back to DataFrame with original columns\n",
    "        self.sensor_data = pd.DataFrame(sensor_data_np, columns=columns)\n",
    "        \n",
    "        return self.sensor_data, columns\n",
    "\n",
    "    def train_model(self):\n",
    "        # Assume machine_status is the target variable for anomaly detection\n",
    "        X = self.sensor_data.drop(columns=['machine_status'])\n",
    "        y = self.sensor_data['machine_status']\n",
    "        \n",
    "        # Train a RandomForestClassifier (or any suitable model)\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Save the trained model\n",
    "        model_filename = \"trained_model.joblib\"\n",
    "        joblib.dump(model, model_filename)\n",
    "        \n",
    "        return model_filename\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
