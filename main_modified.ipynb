{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbdb19c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 02:05:52,751 - INFO - Logging setup complete.\n",
      "2024-07-31 02:05:52,751 - INFO - Starting dataset download...\n",
      "2024-07-31 02:06:00,832 - INFO - Dataset download and extraction completed.\n",
      "2024-07-31 02:06:00,832 - INFO - Dataset path set to: C:/Users/mozhdeh/Desktop/programming 4\\sensor.csv\n",
      "2024-07-31 02:06:00,832 - INFO - Found model file. Running model pipeline...\n",
      "2024-07-31 02:06:00,832 - INFO - Running pipeline\n",
      "2024-07-31 02:06:00,832 - INFO - Loading data from C:/Users/mozhdeh/Desktop/programming 4\\sensor.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded and extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 02:06:17,727 - INFO - Data loaded and saved: train_data.csv, val_data_july.csv, test_data_august.csv\n",
      "2024-07-31 02:06:17,821 - INFO - Data split into training and validation sets.\n",
      "C:\\Users\\mozhdeh\\.vscode\\cli\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "C:\\Users\\mozhdeh\\.vscode\\cli\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "C:\\Users\\mozhdeh\\.vscode\\cli\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "2024-07-31 02:06:17,962 - INFO - Data transformation completed.\n",
      "2024-07-31 02:07:23,813 - INFO - Model trained and saved to C:/Users/mozhdeh/Desktop/programming 4\\model.pkl\n",
      "C:\\Users\\mozhdeh\\.vscode\\cli\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "2024-07-31 02:07:27,243 - INFO - Validation Accuracy: 0.93\n",
      "2024-07-31 02:07:27,243 - INFO - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BROKEN       0.00      0.00      0.00         1\n",
      "      NORMAL       0.93      1.00      0.97     36700\n",
      "  RECOVERING       0.00      0.00      0.00      2611\n",
      "\n",
      "    accuracy                           0.93     39312\n",
      "   macro avg       0.31      0.33      0.32     39312\n",
      "weighted avg       0.87      0.93      0.90     39312\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.93\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      BROKEN       0.00      0.00      0.00         1\n",
      "      NORMAL       0.93      1.00      0.97     36700\n",
      "  RECOVERING       0.00      0.00      0.00      2611\n",
      "\n",
      "    accuracy                           0.93     39312\n",
      "   macro avg       0.31      0.33      0.32     39312\n",
      "weighted avg       0.87      0.93      0.90     39312\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 02:07:29,468 - INFO - Anomaly plot saved to C:/Users/mozhdeh/Desktop/programming 4\\anomaly_plot_sensor_04.png\n",
      "2024-07-31 02:07:31,987 - INFO - Anomaly plot saved to C:/Users/mozhdeh/Desktop/programming 4\\anomaly_plot_sensor_51.png\n",
      "2024-07-31 02:07:31,990 - INFO - Processing new data from C:/Users/mozhdeh/Desktop/programming 4\\test_data_august.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly plot for sensor_04 saved to C:/Users/mozhdeh/Desktop/programming 4\\anomaly_plot_sensor_04.png\n",
      "Anomaly plot for sensor_51 saved to C:/Users/mozhdeh/Desktop/programming 4\\anomaly_plot_sensor_51.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 02:07:33,744 - INFO - Model pipeline execution completed.\n",
      "2024-07-31 02:07:33,744 - INFO - Starting production pipeline...\n",
      "2024-07-31 02:07:33,744 - INFO - Configuration loaded.\n",
      "2024-07-31 02:07:33,744 - INFO - Logging setup complete.\n",
      "2024-07-31 02:07:33,744 - INFO - Started listening for new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for new data:\n",
      " ['NORMAL' 'NORMAL' 'NORMAL' ... 'NORMAL' 'NORMAL' 'NORMAL']\n"
     ]
    }
   ],
   "source": [
    "# The goal of this project is to develop a data pipeline application for anomaly detection using a machine learning model.\n",
    "# the whole process includes:\n",
    "\n",
    "#loading the data from kaggle. Split a dataset into training, validation, and test sets.\n",
    "# Training data: April, May, and June 2018\n",
    "# Validation data: July 2018\n",
    "# Test data: August 2018\n",
    "#Model Training: Train a machine learning model on the training data and save the model to a file.\n",
    "# Data Transformation for trained data\n",
    "# Anomaly Detection: Use the trained model to detect anomalies in the validation and test datasets.\n",
    "# Plotting: Create and save plots of sensor data anomalies. Ensure that the plotting function is independent of global variables and returns the plot file path.\n",
    "# Monitoring New Data: Creating a component that listens to a directory for new files, processes them using the trained model, and performs necessary data transformations.\n",
    "# Output Handling(Logging: Log the processing steps, including any errors that occur, to a log file.)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from dataset_loader import DatasetDownloader\n",
    "from pipeline_model import ModelPipeline\n",
    "from production_pipeline import ProductionPipeline\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"\n",
    "    Set up logging configuration for the application.\n",
    "\n",
    "    Configures the logging to output messages to the console with a specific format.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                        handlers=[logging.StreamHandler()])\n",
    "    logging.info(\"Logging setup complete.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the data pipeline, including dataset download, model training, \n",
    "    and production pipeline monitoring.\n",
    "\n",
    "    Steps:\n",
    "    1. Setup logging configuration.\n",
    "    2. Define file paths and URLs for dataset and model.\n",
    "    3. Download the dataset.\n",
    "    4. Verify the dataset file exists.\n",
    "    5. Initialize and run the model pipeline if the model file is found.\n",
    "    6. Initialize and run the production pipeline if the configuration file is found.\n",
    "    \"\"\"\n",
    "    # Setup logging configuration\n",
    "    setup_logging()\n",
    "\n",
    "    # Defining paths and URLs\n",
    "    credentials_file = 'C:/Users/mozhdeh/Desktop/programming 4/kaggle.json'\n",
    "    api_url = 'https://www.kaggle.com/api/v1/datasets/download/nphantawee/pump-sensor-data'\n",
    "    destination_folder = 'C:/Users/mozhdeh/Desktop/programming 4'\n",
    "    dataset_file_name = 'sensor.csv'\n",
    "    dataset_path = os.path.join(destination_folder, dataset_file_name)\n",
    "    model_path = os.path.join(destination_folder, 'model.pkl')\n",
    "    config_path = 'application.json'\n",
    "\n",
    "    # Creating destination folder if it doesn't exist\n",
    "    os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "    logging.info(\"Starting dataset download...\")\n",
    "\n",
    "    try:\n",
    "        # Creating DatasetDownloader instance from dataset_loader module and download the dataset\n",
    "        downloader = DatasetDownloader(credentials_file, api_url, destination_folder)\n",
    "        downloader.download_dataset()\n",
    "        logging.info(\"Dataset download and extraction completed.\")\n",
    "    except Exception as e:\n",
    "        logging.error(\"An error occurred during dataset download: %s\", e)\n",
    "        return\n",
    "\n",
    "    # Checking if the dataset file exists before proceeding\n",
    "    if not os.path.isfile(dataset_path):\n",
    "        logging.error(\"Dataset file not found at %s\", dataset_path)\n",
    "        return\n",
    "\n",
    "    logging.info(\"Dataset path set to: %s\", dataset_path)\n",
    "\n",
    "    # Initializing and using the ModelPipeline module\n",
    "    if os.path.isfile(model_path):\n",
    "        try:\n",
    "            model_pipeline = ModelPipeline(model_path=model_path, data_dir=destination_folder)\n",
    "            logging.info(\"Found model file. Running model pipeline...\")\n",
    "            model_pipeline.run_pipeline(input_data_path=dataset_path)\n",
    "            logging.info(\"Model pipeline execution completed.\")\n",
    "        except Exception as e:\n",
    "            logging.error(\"An error occurred during model pipeline execution: %s\", e)\n",
    "            return\n",
    "    else:\n",
    "        logging.error(\"Model file not found at %s\", model_path)\n",
    "        return\n",
    "\n",
    "    # Initialize and run the production pipeline module\n",
    "    if os.path.isfile(config_path):\n",
    "        try:\n",
    "            logging.info(\"Starting production pipeline...\")\n",
    "            production_pipeline = ProductionPipeline(config_path=config_path)\n",
    "            \n",
    "            # Start monitoring the input directory for new files\n",
    "            production_pipeline.start_monitoring()\n",
    "            logging.info(\"Production pipeline is now monitoring for new files.\")\n",
    "        except Exception as e:\n",
    "            logging.error(\"An error occurred while running the production pipeline: %s\", e)\n",
    "    else:\n",
    "        logging.error(\"Configuration file not found at %s\", config_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a3c6750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the output shows:\n",
    "#The script has successfully downloaded and extracted the dataset from the specified source.\n",
    "#The path to the dataset is confirmed, and it's ready for further processing.\n",
    "#The script has found the model file and started the model pipeline. It loaded the data from the CSV file, \n",
    "#processed it, and saved it into training, validation, and test sets. The data splitting step is complete,\n",
    "#preparing the data for model training has been done.\n",
    "#The model has achieved perfect classification on the validation set, meaning it made no mistakes on the data it was evaluated against.\n",
    "#This high accuracy (0.93) might suggest that the model is performing exceptionally well on this specific dataset.\n",
    "#he system has successfully generated and saved an anomaly plot for the sensor 'sensor_014,51'. \n",
    "#This plot visualizes anomalies detected in the sensor data and is saved as an image file.\n",
    "#The system has started processing new data from the file test_data_august.csv.\n",
    "#This data will be evaluated by the trained model to make predictions.\n",
    "#the entire model pipeline process (including data preparation, training, evaluation, and possibly anomaly plotting) has been completed successfully\n",
    "#The production pipeline has been initiated. This phase is responsible for applying the trained model to new data in a real-time or batch processing manner.\n",
    "#The production pipeline has successfully loaded its configuration settings, which might include parameters for monitoring directories, model paths, and other operational details.\n",
    "#Logging has been set up for the production pipeline to record operations and any issues that occur during execution.\n",
    "#The production pipeline is now actively monitoring the specified directory for new files. When new data files arrive, they will be processed using the trained model.\n",
    "#The model has made predictions on the new data from test_data_august.csv. All predictions are classified as 'NORMAL', indicating that the model has identified all instances in the new data as belonging to the 'NORMAL' class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d23e1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
