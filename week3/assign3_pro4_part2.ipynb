{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e238b87",
   "metadata": {},
   "source": [
    "# step 0: obtaining an API keyÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a8211b",
   "metadata": {},
   "source": [
    "#the signin-page of the NBCI  : i could not enter this site to get API "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70919b5",
   "metadata": {},
   "source": [
    "# step 1: install and use the biopython package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3327ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Definition: def download_article(article_id, api_key) defines a function named download_article that takes two parameters: article_id (a string representing the PubMed article ID) and api_key (a string representing the user's NCBI API key).\n",
    "# Fetching Data:\n",
    "# handle = Entrez.efetch(db=\"pubmed\", id=article_id, retmode=\"xml\", api_key=api_key): This line uses the efetch function from the Entrez module to fetch the article data from the PubMed database. The data is returned in XML format. The api_key parameter is used for authentication with the NCBI servers.\n",
    "# Reading Data: xml_data = handle.read() reads the data fetched by efetch into a variable xml_data.\n",
    "# Creating Filename: filename = f\"{article_id}.xml\" creates a filename using the article_id, ensuring that each article is saved with a unique name based on its ID.\n",
    "# Writing Data to File:\n",
    "# with open(filename, 'w') as file: Opens a new file in write mode ('w'), with the filename created above.\n",
    "# file.write(xml_data): Writes the fetched XML data to the file.\n",
    "\n",
    "\n",
    "from Bio import Entrez\n",
    "\n",
    "import os\n",
    "\n",
    "def download_article(article_id, api_key):\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=article_id, retmode=\"xml\", api_key=api_key)\n",
    "    xml_data = handle.read()\n",
    "    filename = f\"{article_id}.xml\"\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(xml_data)\n",
    "\n",
    "        #since i did not have api_key I have no output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc5f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def parallel_download(article_ids, api_key):\n",
    "    with mp.Pool() as pool:\n",
    "        pool.starmap(download_article, [(article_id, api_key) for article_id in article_ids])\n",
    "# #The parallel_download function uses the multiprocessing module to download articles in parallel.\n",
    "# It creates a pool of worker processes and uses the starmap method to execute the download_article function with multiple arguments.\n",
    "# The function distributes the task of downloading articles across multiple processes, improving efficiency and reducing download time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b84a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    Entrez.email = '<YOUR EMAIL HERE>'\n",
    "    api_key = '<YOUR API KEY HERE>'\n",
    "    pubmed_id = '30049270'\n",
    "    \n",
    "    file = Entrez.elink(dbfrom=\"pubmed\", db=\"pmc\", LinkName=\"pubmed_pmc_refs\", id=pubmed_id, api_key=api_key)\n",
    "    results = Entrez.read(file)\n",
    "    references = [f'{link[\"Id\"]}' for link in results[0][\"LinkSetDb\"][0][\"Link\"]][:10]  # Limit to 10\n",
    "\n",
    "    parallel_download(references, api_key)\n",
    "# use of the above function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d8e0a7",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43842c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-Threaded Approach\n",
    "#Define the Sequential Download Function:\n",
    "#This function will download and save articles one by one.\n",
    "\n",
    "from Biopython import Entrez\n",
    "\n",
    "def download_article_single(article_id, api_key):\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=article_id, retmode=\"xml\", api_key=api_key)\n",
    "    xml_data = handle.read()\n",
    "    filename = f\"{article_id}.xml\"\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(xml_data)\n",
    "        \n",
    "# The download_article_single function is a single-threaded approach for downloading and saving articles one by one.\n",
    "# It uses the Biopython Entrez module to fetch article data from PubMed in XML format.\n",
    "# The function reads the data,creates a filename based on the article ID, and saves the XML data to a file.\n",
    "# This approach processes each article sequentially,one at a time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce2fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing the Sequential Download Logic.\n",
    "#Fetch references and download articles sequentially.\n",
    "import time\n",
    "from Bio import Entrez\n",
    "\n",
    "def sequential_download(article_ids, api_key):\n",
    "    for article_id in article_ids:\n",
    "        download_article_single(article_id, api_key)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Entrez.email = '<YOUR EMAIL HERE>'\n",
    "    api_key = '<YOUR API KEY HERE>'\n",
    "    pubmed_id = '30049270'\n",
    "    \n",
    "    # Fetch references\n",
    "    file = Entrez.elink(dbfrom=\"pubmed\", db=\"pmc\", LinkName=\"pubmed_pmc_refs\", id=pubmed_id, api_key=api_key)\n",
    "    results = Entrez.read(file)\n",
    "    references = [f'{link[\"Id\"]}' for link in results[0][\"LinkSetDb\"][0][\"Link\"]][:10]  # Limit to 10\n",
    "\n",
    "    # Measure time for sequential download\n",
    "    start_time = time.time()\n",
    "    sequential_download(references, api_key)\n",
    "    end_time = time.time()\n",
    "    print(f\"Sequential download took {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1154f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Multi-Threaded Approach\n",
    "#Defining the Parallel Download Function.\n",
    "# this function will use multiprocessing to download articles concurrently.\n",
    "import multiprocessing as mp\n",
    "from Bio import Entrez\n",
    "\n",
    "def download_article(article_id, api_key):\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=article_id, retmode=\"xml\", api_key=api_key)\n",
    "    xml_data = handle.read()\n",
    "    filename = f\"{article_id}.xml\"\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(xml_data)\n",
    "\n",
    "def parallel_download(article_ids, api_key):\n",
    "    with mp.Pool() as pool:\n",
    "        pool.starmap(download_article, [(article_id, api_key) for article_id in article_ids])\n",
    "\n",
    "        \n",
    "#This function processes articles one at a time in a sequential manner, which means it handles each article individually \n",
    "#and waits until one article is fully processed before moving on to the next. This approach is straightforward but may not\n",
    "#be efficient for handling a large number of articles due to the sequential nature of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e7044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement Parallel Download Logic:\n",
    "#Fetch references and download articles using multiprocessing.\n",
    "\n",
    "\n",
    "import time\n",
    "from Bio import Entrez\n",
    "import multiprocessing as mp\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Entrez.email = '<YOUR EMAIL HERE>'\n",
    "    api_key = '<YOUR API KEY HERE>'\n",
    "    pubmed_id = '30049270'\n",
    "    \n",
    "    # Fetch references\n",
    "    file = Entrez.elink(dbfrom=\"pubmed\", db=\"pmc\", LinkName=\"pubmed_pmc_refs\", id=pubmed_id, api_key=api_key)\n",
    "    results = Entrez.read(file)\n",
    "    references = [f'{link[\"Id\"]}' for link in results[0][\"LinkSetDb\"][0][\"Link\"]][:10]  # Limit to 10\n",
    "\n",
    "    # Measure time for parallel download\n",
    "    start_time = time.time()\n",
    "    parallel_download(references, api_key)\n",
    "    end_time = time.time()\n",
    "    print(f\"Parallel download took {end_time - start_time} seconds\")\n",
    "#This script demonstrates how to efficiently download a large number of articles by leveraging parallel\n",
    "#processing to speed up the data retrieval process.\n",
    "\n",
    "\n",
    "\n",
    "#it includes:\n",
    "# Setup: Import necessary modules and define parameters such as email, API key, and PubMed ID.\n",
    "# Fetch References: Query PubMed to get related article IDs and limit to the first 10.\n",
    "# Parallel Download: Use multiprocessing to download the articles concurrently.\n",
    "# Measure Time: Calculate and print the time taken for the parallel download to assess performance improvements.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
