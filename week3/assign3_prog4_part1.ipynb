{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb4b2ba0",
   "metadata": {},
   "source": [
    "# part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7019d301",
   "metadata": {},
   "source": [
    "# The goal of this assignment is:\n",
    "\n",
    "Enhancing Code Quality: Through refactoring and use of Pythonic constructs such as list comprehensions, iterators, and generators, improve the maintainability and readability of the code.\n",
    "\n",
    "Applying SOLID Principles: Refactor the code to adhere to SOLID principles, ensuring better organization, flexibility, and scalability of the codebase.\n",
    "\n",
    "Improving Functionality and Control: By changing the loop into an iterator and using generators, you gain better control over the execution flow and resource management of the crawler.\n",
    "\n",
    "Document and Communicate: Provide a clear analysis of the refactoring done and potential improvements based on design principles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ad144d",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ebfc1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "def apply_function_to_data(data, func):\n",
    "    \"\"\"\n",
    "    Apply a function to each element in a list of data.\n",
    "\n",
    "    Parameters:\n",
    "    data (list): A list of data elements.\n",
    "    func (function): A function to apply to each data element.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of new values obtained by applying the function to each data element.\n",
    "    \"\"\"\n",
    "    return [func(x) for x in data]\n",
    "\n",
    "# Example usage:\n",
    "data = [1, 2, 3, 4, 5]\n",
    "func = lambda x: x * 2\n",
    "\n",
    "result = apply_function_to_data(data, func)\n",
    "print(result)  # Output: [2, 4, 6, 8, 10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "904af0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 4, 6, 8, 10], [2, 3, 4, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "def apply_functions(data,*funcs):\n",
    "    \"\"\"\n",
    "    Applying one or more functions to a list of data.\n",
    "    Parameters:\n",
    "    data(list): A list of data elements.\n",
    "    *funcs(function): One or more functions to apply to the data.\n",
    "    \n",
    "    Returns:\n",
    "    list:list of lists containing the results of applying each function to the data.\n",
    "    \"\"\"\n",
    "    return [[func(x) for x in data] for func in funcs]\n",
    "\n",
    "# bellow is an example of using the above function\n",
    "data = [1, 2, 3, 4, 5]\n",
    "func1= lambda x: x * 2\n",
    "func2= lambda x: x + 1\n",
    "\n",
    "result = apply_functions(data, func1, func2)\n",
    "print(result)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926239d8",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c1e1866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetch urls\n",
      "HTTP Error: 404 - Not Found\n",
      "Failed to retrieve the URL. Exiting.\n"
     ]
    }
   ],
   "source": [
    "# the code has been  already provided in assignment\n",
    "\n",
    "import urllib.request\n",
    "import ssl\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def hack_ssl():\n",
    "    \"\"\"Ignores the certificate errors\"\"\"\n",
    "    ctx = ssl.create_default_context()\n",
    "    ctx.check_hostname = False\n",
    "    ctx.verify_mode = ssl.CERT_NONE\n",
    "    return ctx\n",
    "\n",
    "def open_url(url):\n",
    "    \"\"\"Reads URL file as a big string and cleans the HTML file to make it more readable. Input: URL, output: soup object\"\"\"\n",
    "    ctx = hack_ssl()\n",
    "    try:\n",
    "        response = urllib.request.urlopen(url, context=ctx)\n",
    "        html = response.read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        return soup\n",
    "    except urllib.error.HTTPError as e:\n",
    "        print(f\"HTTP Error: {e.code} - {e.reason}\")\n",
    "        return None\n",
    "    except urllib.error.URLError as e:\n",
    "        print(f\"URL Error: {e.reason}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def read_hrefs(soup):\n",
    "    \"\"\"Get from soup object a list of anchor tags, get the href keys. Input: soup object\"\"\"\n",
    "    reflist = []\n",
    "    tags = soup('a')\n",
    "    for tag in tags:\n",
    "        reflist.append(tag)\n",
    "    return reflist\n",
    "\n",
    "def read_li(soup):\n",
    "    \"\"\"Get from soup object a list of list items. Input: soup object\"\"\"\n",
    "    lilist = []\n",
    "    tags = soup('li')\n",
    "    for tag in tags:\n",
    "        lilist.append(tag)\n",
    "    return lilist\n",
    "\n",
    "def get_phone(info):\n",
    "    \"\"\"Extracts phone number from the list of information. Input: list of BeautifulSoup objects\"\"\"\n",
    "    reg = r\"(?:(?:00|\\+)?[0-9]{4})?(?:[ .-][0-9]{3}){1,5}\"\n",
    "    phone = [s for s in filter(lambda x: 'Telefoon' in str(x), info)]\n",
    "    try:\n",
    "        phone = str(phone[0])\n",
    "    except:\n",
    "        phone = [s for s in filter(lambda x: re.findall(reg, str(x)), info)]\n",
    "        try:\n",
    "            phone = str(phone[0])\n",
    "        except:\n",
    "            phone = \"\"\n",
    "    return phone.replace('Facebook', '').replace('Telefoon:', '')\n",
    "\n",
    "def get_email(soup):\n",
    "    \"\"\"Extracts email from the soup object. Input: soup object\"\"\"\n",
    "    try:\n",
    "        email = [s for s in filter(lambda x: '@' in str(x), soup)]\n",
    "        email = str(email[0])[4:-5]\n",
    "        bs = BeautifulSoup(email, features=\"html.parser\")\n",
    "        email = bs.find('a').attrs['href'].replace('mailto:', '')\n",
    "    except:\n",
    "        email = \"\"\n",
    "    return email\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove HTML tags from a string\"\"\"\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "def fetch_sidebar(soup):\n",
    "    \"\"\"Reads HTML file and extracts sidebar. Input: HTML, output: sidebar\"\"\"\n",
    "    sidebar = soup.findAll(attrs={'class': 'sidebar'})\n",
    "    if sidebar:\n",
    "        return sidebar[0]\n",
    "    return None\n",
    "\n",
    "def extract(url):\n",
    "    \"\"\"Extracts and formats the URL part needed\"\"\"\n",
    "    text = str(url)\n",
    "    text = text.split('\"')[0] + \"/\"\n",
    "    return text\n",
    "\n",
    "def main(url):\n",
    "    \"\"\"Main function to execute the script.\"\"\"\n",
    "    print('fetch urls')\n",
    "    s = open_url(url)\n",
    "    if s is None:\n",
    "        print(\"Failed to retrieve the URL. Exiting.\")\n",
    "        return\n",
    "\n",
    "    reflist = read_hrefs(s)\n",
    "\n",
    "    # Print some of the fetched hrefs for inspection\n",
    "    for tag in reflist[:10]:  # Print first 10 for inspection\n",
    "        print(tag.get('href'))\n",
    "\n",
    "    print('getting sub-urls')\n",
    "    sub_urls = [tag.get('href') for tag in reflist if '/sportaanbieders' in tag.get('href', '')]\n",
    "    sub_urls = sub_urls[3:]\n",
    "\n",
    "    print('extracting the data')\n",
    "    print(f'{len(sub_urls)} sub-urls')\n",
    "\n",
    "    for sub in sub_urls:\n",
    "        try:\n",
    "            sub = extract(sub)\n",
    "            site = url.rstrip('/') + sub\n",
    "            print(f\"Fetching site: {site}\")\n",
    "            soup = open_url(site)\n",
    "            if soup is None:\n",
    "                print(f\"Failed to retrieve the site: {site}. Skipping.\")\n",
    "                continue\n",
    "            info = fetch_sidebar(soup)\n",
    "            if info is None:\n",
    "                print(f\"No sidebar found for site: {site}. Skipping.\")\n",
    "                continue\n",
    "            info = read_li(info)\n",
    "            phone = get_phone(info)\n",
    "            phone = remove_html_tags(phone).strip()\n",
    "            email = get_email(info)\n",
    "            email = remove_html_tags(email).replace(\"/\", \"\")\n",
    "            print(f'{site} ; {phone} ; {email}')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {sub}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"https://sport050.nl/sportaanbieders/alle-aanbieders/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e803fb",
   "metadata": {},
   "source": [
    "# Step 1: Simple refactor improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "496cf5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetch urls\n",
      "HTTP Error: 404 - Not Found\n",
      "Failed to retrieve the URL. Exiting.\n"
     ]
    }
   ],
   "source": [
    "# we have refactored the existing web crawler code into a class called Crawler to improve maintainability and reusability.\n",
    "#The main logic of the program has been encapsulated in a method named crawl_site().\n",
    "\n",
    "import urllib.request\n",
    "import ssl\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "\n",
    "    def hack_ssl(self):\n",
    "        \"\"\"Ignores the certificate errors\"\"\"\n",
    "        ctx = ssl.create_default_context()\n",
    "        ctx.check_hostname = False\n",
    "        ctx.verify_mode = ssl.CERT_NONE\n",
    "        return ctx\n",
    "\n",
    "    def open_url(self, url):\n",
    "        \"\"\"Reads URL file as a big string and cleans the HTML file to make it more readable. Input: URL, output: soup object\"\"\"\n",
    "        ctx = self.hack_ssl()\n",
    "        try:\n",
    "            response = urllib.request.urlopen(url, context=ctx)\n",
    "            html = response.read()\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            return soup\n",
    "        except urllib.error.HTTPError as e:\n",
    "            print(f\"HTTP Error: {e.code} - {e.reason}\")\n",
    "            return None\n",
    "        except urllib.error.URLError as e:\n",
    "            print(f\"URL Error: {e.reason}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "    def read_hrefs(self, soup):\n",
    "        \"\"\"Get from soup object a list of anchor tags, get the href keys. Input: soup object\"\"\"\n",
    "        reflist = []\n",
    "        tags = soup('a')\n",
    "        for tag in tags:\n",
    "            reflist.append(tag)\n",
    "        return reflist\n",
    "\n",
    "    def read_li(self, soup):\n",
    "        \"\"\"Get from soup object a list of list items. Input: soup object\"\"\"\n",
    "        lilist = []\n",
    "        tags = soup('li')\n",
    "        for tag in tags:\n",
    "            lilist.append(tag)\n",
    "        return lilist\n",
    "\n",
    "    def get_phone(self, info):\n",
    "        \"\"\"Extracts phone number from the list of information. Input: list of BeautifulSoup objects\"\"\"\n",
    "        reg = r\"(?:(?:00|\\+)?[0-9]{4})?(?:[ .-][0-9]{3}){1,5}\"\n",
    "        phone = [s for s in filter(lambda x: 'Telefoon' in str(x), info)]\n",
    "        try:\n",
    "            phone = str(phone[0])\n",
    "        except:\n",
    "            phone = [s for s in filter(lambda x: re.findall(reg, str(x)), info)]\n",
    "            try:\n",
    "                phone = str(phone[0])\n",
    "            except:\n",
    "                phone = \"\"\n",
    "        return phone.replace('Facebook', '').replace('Telefoon:', '')\n",
    "\n",
    "    def get_email(self, soup):\n",
    "        \"\"\"Extracts email from the soup object. Input: soup object\"\"\"\n",
    "        try:\n",
    "            email = [s for s in filter(lambda x: '@' in str(x), soup)]\n",
    "            email = str(email[0])[4:-5]\n",
    "            bs = BeautifulSoup(email, features=\"html.parser\")\n",
    "            email = bs.find('a').attrs['href'].replace('mailto:', '')\n",
    "        except:\n",
    "            email = \"\"\n",
    "        return email\n",
    "\n",
    "    def remove_html_tags(self, text):\n",
    "        \"\"\"Remove HTML tags from a string\"\"\"\n",
    "        clean = re.compile('<.*?>')\n",
    "        return re.sub(clean, '', text)\n",
    "\n",
    "    def fetch_sidebar(self, soup):\n",
    "        \"\"\"Reads HTML file and extracts sidebar. Input: HTML, output: sidebar\"\"\"\n",
    "        sidebar = soup.findAll(attrs={'class': 'sidebar'})\n",
    "        if sidebar:\n",
    "            return sidebar[0]\n",
    "        return None\n",
    "\n",
    "    def extract(self, url):\n",
    "        \"\"\"Extracts and formats the URL part needed\"\"\"\n",
    "        text = str(url)\n",
    "        text = text.split('\"')[0] + \"/\"\n",
    "        return text\n",
    "\n",
    "    def crawl_site(self):\n",
    "        \"\"\"Main function to execute the script.\"\"\"\n",
    "        print('fetch urls')\n",
    "        s = self.open_url(self.base_url)\n",
    "        if s is None:\n",
    "            print(\"Failed to retrieve the URL. Exiting.\")\n",
    "            return\n",
    "\n",
    "        reflist = self.read_hrefs(s)\n",
    "\n",
    "        # Print some of the fetched hrefs for inspection\n",
    "        for tag in reflist[:10]:  # Print first 10 for inspection\n",
    "            print(tag.get('href'))\n",
    "\n",
    "        print('getting sub-urls')\n",
    "        sub_urls = [tag.get('href') for tag in reflist if '/sportaanbieders' in tag.get('href', '')]\n",
    "        sub_urls = sub_urls[3:]\n",
    "\n",
    "        print('extracting the data')\n",
    "        print(f'{len(sub_urls)} sub-urls')\n",
    "\n",
    "        for sub in sub_urls:\n",
    "            try:\n",
    "                sub = self.extract(sub)\n",
    "                site = self.base_url.rstrip('/') + sub\n",
    "                print(f\"Fetching site: {site}\")\n",
    "                soup = self.open_url(site)\n",
    "                if soup is None:\n",
    "                    print(f\"Failed to retrieve the site: {site}. Skipping.\")\n",
    "                    continue\n",
    "                info = self.fetch_sidebar(soup)\n",
    "                if info is None:\n",
    "                    print(f\"No sidebar found for site: {site}. Skipping.\")\n",
    "                    continue\n",
    "                info = self.read_li(info)\n",
    "                phone = self.get_phone(info)\n",
    "                phone = self.remove_html_tags(phone).strip()\n",
    "                email = self.get_email(info)\n",
    "                email = self.remove_html_tags(email).replace(\"/\", \"\")\n",
    "                print(f'{site} ; {phone} ; {email}')\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing {sub}: {e}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    url = \"https://sport050.nl/sportaanbieders/alle-aanbieders/\"\n",
    "    crawler = Crawler(url)\n",
    "    crawler.crawl_site()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "581cc1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetch urls\n",
      "HTTP Error: 404 - Not Found\n",
      "Failed to retrieve the URL. Exiting.\n"
     ]
    }
   ],
   "source": [
    "# REMOVING LAMBDA from class to impove the readability\n",
    "import urllib.request\n",
    "import ssl\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "\n",
    "    def hack_ssl(self):\n",
    "        \"\"\"Ignores the certificate errors\"\"\"\n",
    "        ctx = ssl.create_default_context()\n",
    "        ctx.check_hostname = False\n",
    "        ctx.verify_mode = ssl.CERT_NONE\n",
    "        return ctx\n",
    "\n",
    "    def open_url(self, url):\n",
    "        \"\"\"Reads URL file as a big string and cleans the HTML file to make it more readable. Input: URL, output: soup object\"\"\"\n",
    "        ctx = self.hack_ssl()\n",
    "        try:\n",
    "            response = urllib.request.urlopen(url, context=ctx)\n",
    "            html = response.read()\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            return soup\n",
    "        except urllib.error.HTTPError as e:\n",
    "            print(f\"HTTP Error: {e.code} - {e.reason}\")\n",
    "            return None\n",
    "        except urllib.error.URLError as e:\n",
    "            print(f\"URL Error: {e.reason}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "    def read_hrefs(self, soup):\n",
    "        \"\"\"Get from soup object a list of anchor tags, get the href keys. Input: soup object\"\"\"\n",
    "        tags = soup('a')\n",
    "        reflist = [tag for tag in tags]\n",
    "        return reflist\n",
    "\n",
    "    def read_li(self, soup):\n",
    "        \"\"\"Get from soup object a list of list items. Input: soup object\"\"\"\n",
    "        tags = soup('li')\n",
    "        lilist = [tag for tag in tags]\n",
    "        return lilist\n",
    "\n",
    "    def get_phone(self, info):\n",
    "        \"\"\"Extracts phone number from the list of information. Input: list of BeautifulSoup objects\"\"\"\n",
    "        reg = r\"(?:(?:00|\\+)?[0-9]{4})?(?:[ .-][0-9]{3}){1,5}\"\n",
    "        phone = [s for s in info if 'Telefoon' in str(s)]\n",
    "        if phone:\n",
    "            phone = str(phone[0])\n",
    "        else:\n",
    "            phone = [s for s in info if re.findall(reg, str(s))]\n",
    "            if phone:\n",
    "                phone = str(phone[0])\n",
    "            else:\n",
    "                phone = \"\"\n",
    "        return phone.replace('Facebook', '').replace('Telefoon:', '')\n",
    "\n",
    "    def get_email(self, soup):\n",
    "        \"\"\"Extracts email from the soup object. Input: soup object\"\"\"\n",
    "        email_tags = [s for s in soup if '@' in str(s)]\n",
    "        if email_tags:\n",
    "            email = str(email_tags[0])[4:-5]\n",
    "            bs = BeautifulSoup(email, features=\"html.parser\")\n",
    "            email = bs.find('a').attrs['href'].replace('mailto:', '')\n",
    "        else:\n",
    "            email = \"\"\n",
    "        return email\n",
    "\n",
    "    def remove_html_tags(self, text):\n",
    "        \"\"\"Remove HTML tags from a string\"\"\"\n",
    "        clean = re.compile('<.*?>')\n",
    "        return re.sub(clean, '', text)\n",
    "\n",
    "    def fetch_sidebar(self, soup):\n",
    "        \"\"\"Reads HTML file and extracts sidebar. Input: HTML, output: sidebar\"\"\"\n",
    "        sidebar = soup.findAll(attrs={'class': 'sidebar'})\n",
    "        if sidebar:\n",
    "            return sidebar[0]\n",
    "        return None\n",
    "\n",
    "    def extract(self, url):\n",
    "        \"\"\"Extracts and formats the URL part needed\"\"\"\n",
    "        text = str(url)\n",
    "        text = text.split('\"')[0] + \"/\"\n",
    "        return text\n",
    "\n",
    "    def crawl_site(self):\n",
    "        \"\"\"Main function to execute the script.\"\"\"\n",
    "        print('fetch urls')\n",
    "        s = self.open_url(self.base_url)\n",
    "        if s is None:\n",
    "            print(\"Failed to retrieve the URL. Exiting.\")\n",
    "            return\n",
    "\n",
    "        reflist = self.read_hrefs(s)\n",
    "\n",
    "        # Print some of the fetched hrefs for inspection\n",
    "        for tag in reflist[:10]:  # Print first 10 for inspection\n",
    "            print(tag.get('href'))\n",
    "\n",
    "        print('getting sub-urls')\n",
    "        sub_urls = [tag.get('href') for tag in reflist if '/sportaanbieders' in tag.get('href', '')]\n",
    "        sub_urls = sub_urls[3:]\n",
    "\n",
    "        print('extracting the data')\n",
    "        print(f'{len(sub_urls)} sub-urls')\n",
    "\n",
    "        for sub in sub_urls:\n",
    "            try:\n",
    "                sub = self.extract(sub)\n",
    "                site = self.base_url.rstrip('/') + sub\n",
    "                print(f\"Fetching site: {site}\")\n",
    "                soup = self.open_url(site)\n",
    "                if soup is None:\n",
    "                    print(f\"Failed to retrieve the site: {site}. Skipping.\")\n",
    "                    continue\n",
    "                info = self.fetch_sidebar(soup)\n",
    "                if info is None:\n",
    "                    print(f\"No sidebar found for site: {site}. Skipping.\")\n",
    "                    continue\n",
    "                info = self.read_li(info)\n",
    "                phone = self.get_phone(info)\n",
    "                phone = self.remove_html_tags(phone).strip()\n",
    "                email = self.get_email(info)\n",
    "                email = self.remove_html_tags(email).replace(\"/\", \"\")\n",
    "                print(f'{site} ; {phone} ; {email}')\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing {sub}: {e}\")\n",
    "                continue\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to create a Crawler instance and start crawling.\"\"\"\n",
    "    base_url = \"https://sport050.nl/sportaanbieders/alle-aanbieders/\"\n",
    "    crawler = Crawler(base_url)\n",
    "    crawler.crawl_site()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# read_hrefs Method: Replaced the lambda expression with a list comprehension:\n",
    "\n",
    "# reflist = [tag for tag in tags]\n",
    "# read_li Method: Replaced the lambda expression with a list comprehension:\n",
    "\n",
    "\n",
    "# lilist = [tag for tag in tags]\n",
    "# get_phone Method: Used list comprehensions to filter the info list:\n",
    "\n",
    "\n",
    "# phone = [s for s in info if 'Telefoon' in str(s)]\n",
    "# phone = [s for s in info if re.findall(reg, str(s))]\n",
    "\n",
    "# get_email Method: Used a list comprehension to filter email tags:\n",
    "# email_tags = [s for s in soup if '@' in str(s)]\n",
    "# By using list comprehensions instead of lambda expressions, the code becomes more readable and Pythonic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07bc30c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching URLs\n",
      "HTTPError: 404 for URL: https://sport050.nl/sportaanbieders/alle-aanbieders/\n",
      "Failed to fetch the main URL. Exiting.\n"
     ]
    }
   ],
   "source": [
    "# we have removed the lambda expressions,replacing them with regular functions.\n",
    "#Here is the final structure and the necessary changes made:\n",
    "\n",
    "\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "import re\n",
    "import csv\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://sport050.nl/sportaanbieders/alle-aanbieders/\"\n",
    "\n",
    "    @staticmethod\n",
    "    def hack_ssl():\n",
    "        \"\"\"Ignores the certificate errors\"\"\"\n",
    "        ctx = ssl.create_default_context()\n",
    "        ctx.check_hostname = False\n",
    "        ctx.verify_mode = ssl.CERT_NONE\n",
    "        return ctx\n",
    "\n",
    "    def open_url(self, url):\n",
    "        \"\"\"Reads URL file as a big string and cleans the HTML file to make it more readable. Input: URL, output: soup object\"\"\"\n",
    "        ctx = self.hack_ssl()\n",
    "        try:\n",
    "            html = urllib.request.urlopen(url, context=ctx).read()\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            return soup\n",
    "        except urllib.error.HTTPError as e:\n",
    "            print(f'HTTPError: {e.code} for URL: {url}')\n",
    "            return None\n",
    "        except urllib.error.URLError as e:\n",
    "            print(f'URLError: {e.reason} for URL: {url}')\n",
    "            return None\n",
    "\n",
    "    def read_hrefs(self, soup):\n",
    "        \"\"\"Gets from soup object a list of anchor tags with provider links, gets the href keys and returns them. Input: soup object\"\"\"\n",
    "        tags = soup.select('a.provider-link')\n",
    "        return [tag.get('href') for tag in tags]\n",
    "\n",
    "    def read_li(self, soup):\n",
    "        return [tag for tag in soup('li')]\n",
    "\n",
    "    def get_phone(self, info):\n",
    "        reg = r\"(?:(?:00|\\+)?[0-9]{4})?(?:[ .-][0-9]{3}){1,5}\"\n",
    "        phone = self.filter_phone(info)\n",
    "        if phone:\n",
    "            phone = str(phone[0])\n",
    "        else:\n",
    "            phone = self.filter_reg(info, reg)\n",
    "            if phone:\n",
    "                phone = str(phone[0])\n",
    "            else:\n",
    "                phone = \"\"   \n",
    "        return phone.replace('Facebook', '').replace('Telefoon:', '')\n",
    "\n",
    "    def filter_phone(self, info):\n",
    "        return [s for s in info if 'Telefoon' in str(s)]\n",
    "\n",
    "    def filter_reg(self, info, reg):\n",
    "        return [s for s in info if re.findall(reg, str(s))]\n",
    "\n",
    "    def get_email(self, soup):\n",
    "        try: \n",
    "            email = self.filter_email(soup)\n",
    "            email = str(email[0])[4:-5]\n",
    "            bs = BeautifulSoup(email, features=\"html.parser\")\n",
    "            email = bs.find('a').attrs['href'].replace('mailto:', '')\n",
    "        except:\n",
    "            email = \"\"\n",
    "        return email\n",
    "\n",
    "    def filter_email(self, soup):\n",
    "        return [s for s in soup if '@' in str(s)]\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_html_tags(text):\n",
    "        \"\"\"Remove HTML tags from a string\"\"\"\n",
    "        clean = re.compile('<.*?>')\n",
    "        return re.sub(clean, '', text)\n",
    "\n",
    "    def fetch_sidebar(self, soup):\n",
    "        \"\"\"Reads HTML file as a big string and cleans the HTML file to make it more readable. Input: HTML, output: tables\"\"\"\n",
    "        sidebar = soup.findAll(attrs={'class': 'sidebar'})\n",
    "        return sidebar[0]\n",
    "\n",
    "    def extract(self, url):\n",
    "        text = str(url)\n",
    "        text = text[26:].split('\"')[0] + \"/\"\n",
    "        return text\n",
    "\n",
    "    def crawl_site(self):\n",
    "        print('Fetching URLs')\n",
    "        s = self.open_url(self.base_url)\n",
    "        if s is None:\n",
    "            print(\"Failed to fetch the main URL. Exiting.\")\n",
    "            return\n",
    "\n",
    "        reflist = self.read_hrefs(s)\n",
    "\n",
    "        print('Getting sub-URLs')\n",
    "        sub_urls = reflist\n",
    "        print(f'{len(sub_urls)} sub-URLs')\n",
    "\n",
    "        with open('sport_suppliers.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file, delimiter=';')\n",
    "            writer.writerow(['URL', 'Phone Number', 'Email Address'])\n",
    "\n",
    "            for sub in sub_urls:\n",
    "                try:\n",
    "                    site = self.base_url[:-1] + sub\n",
    "                    soup = self.open_url(site)\n",
    "                    if soup is None:\n",
    "                        continue\n",
    "\n",
    "                    info = self.fetch_sidebar(soup)\n",
    "                    info = self.read_li(info)\n",
    "                    phone = self.get_phone(info)\n",
    "                    phone = self.remove_html_tags(phone).strip()\n",
    "                    email = self.get_email(info)\n",
    "                    email = self.remove_html_tags(email).replace(\"/\", \"\")\n",
    "                    writer.writerow([site, phone, email])\n",
    "                    print(f'{site} ; {phone} ; {email}')\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    crawler = Crawler()\n",
    "    crawler.crawl_site()\n",
    "\n",
    "    \n",
    "# In this class i have changed \n",
    "# phone = [s for s in filter(lambda x: 'Telefoon' in str(x), info)] to \n",
    "# def filter_phone(self, info):\n",
    "#     return [s for s in info if 'Telefoon' in str(s)]\n",
    "\n",
    "# phone = [s for s in filter(lambda x: re.findall(reg, str(x)), info)] to\n",
    "# def filter_reg(self, info, reg):\n",
    "#     return [s for s in info if re.findall(reg, str(s))]\n",
    "\n",
    "# email = [s for s in filter(lambda x: '@' in str(x), soup)] to \n",
    "# def filter_email(self, soup):\n",
    "#     return [s for s in soup if '@' in str(s)]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f8a0d0",
   "metadata": {},
   "source": [
    "#  Step 2: Change the loop into an iteratorÂ¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b276809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetch urls\n",
      "HTTP Error: 404 - Not Found\n",
      "Failed to retrieve the URL. Exiting.\n"
     ]
    }
   ],
   "source": [
    "# here we change loops into iterators\n",
    "import urllib.request\n",
    "import ssl\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "\n",
    "    def hack_ssl(self):\n",
    "        \"\"\"Ignores the certificate errors\"\"\"\n",
    "        ctx = ssl.create_default_context()\n",
    "        ctx.check_hostname = False\n",
    "        ctx.verify_mode = ssl.CERT_NONE\n",
    "        return ctx\n",
    "\n",
    "    def open_url(self, url):\n",
    "        \"\"\"Reads URL file as a big string and cleans the HTML file to make it more readable. Input: URL, output: soup object\"\"\"\n",
    "        ctx = self.hack_ssl()\n",
    "        try:\n",
    "            response = urllib.request.urlopen(url, context=ctx)\n",
    "            html = response.read()\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            return soup\n",
    "        except urllib.error.HTTPError as e:\n",
    "            print(f\"HTTP Error: {e.code} - {e.reason}\")\n",
    "            return None\n",
    "        except urllib.error.URLError as e:\n",
    "            print(f\"URL Error: {e.reason}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "    def read_hrefs(self, soup):\n",
    "        \"\"\"Get from soup object a list of anchor tags, get the href keys. Input: soup object\"\"\"\n",
    "        return [tag for tag in soup('a')]\n",
    "\n",
    "    def read_li(self, soup):\n",
    "        \"\"\"Get from soup object a list of list items. Input: soup object\"\"\"\n",
    "        return [tag for tag in soup('li')]\n",
    "\n",
    "    def get_phone(self, info):\n",
    "        \"\"\"Extracts phone number from the list of information. Input: list of BeautifulSoup objects\"\"\"\n",
    "        reg = r\"(?:(?:00|\\+)?[0-9]{4})?(?:[ .-][0-9]{3}){1,5}\"\n",
    "        phone = [s for s in info if 'Telefoon' in str(s)]\n",
    "        if phone:\n",
    "            phone = str(phone[0])\n",
    "        else:\n",
    "            phone = [s for s in info if re.findall(reg, str(s))]\n",
    "            if phone:\n",
    "                phone = str(phone[0])\n",
    "            else:\n",
    "                phone = \"\"\n",
    "        return phone.replace('Facebook', '').replace('Telefoon:', '')\n",
    "\n",
    "    def get_email(self, soup):\n",
    "        \"\"\"Extracts email from the soup object. Input: soup object\"\"\"\n",
    "        try:\n",
    "            email = [s for s in soup if '@' in str(s)]\n",
    "            if email:\n",
    "                email = str(email[0])[4:-5]\n",
    "                bs = BeautifulSoup(email, features=\"html.parser\")\n",
    "                email = bs.find('a').attrs['href'].replace('mailto:', '')\n",
    "            else:\n",
    "                email = \"\"\n",
    "        except:\n",
    "            email = \"\"\n",
    "        return email\n",
    "\n",
    "    def remove_html_tags(self, text):\n",
    "        \"\"\"Remove HTML tags from a string\"\"\"\n",
    "        clean = re.compile('<.*?>')\n",
    "        return re.sub(clean, '', text)\n",
    "\n",
    "    def fetch_sidebar(self, soup):\n",
    "        \"\"\"Reads HTML file and extracts sidebar. Input: HTML, output: sidebar\"\"\"\n",
    "        sidebar = soup.findAll(attrs={'class': 'sidebar'})\n",
    "        if sidebar:\n",
    "            return sidebar[0]\n",
    "        return None\n",
    "\n",
    "    def extract(self, url):\n",
    "        \"\"\"Extracts and formats the URL part needed\"\"\"\n",
    "        text = str(url)\n",
    "        return text.split('\"')[0] + \"/\"\n",
    "\n",
    "    class SubUrlIterator:\n",
    "        def __init__(self, hrefs):\n",
    "            self.hrefs = hrefs\n",
    "            self.index = 0\n",
    "\n",
    "        def __iter__(self):\n",
    "            return self\n",
    "\n",
    "        def __next__(self):\n",
    "            if self.index >= len(self.hrefs):\n",
    "                raise StopIteration\n",
    "            href = self.hrefs[self.index]\n",
    "            self.index += 1\n",
    "            return href\n",
    "\n",
    "    def crawl_site(self):\n",
    "        \"\"\"Main method to start crawling.\"\"\"\n",
    "        print('fetch urls')\n",
    "        soup = self.open_url(self.base_url)\n",
    "        if soup is None:\n",
    "            print(\"Failed to retrieve the URL. Exiting.\")\n",
    "            return\n",
    "\n",
    "        reflist = self.read_hrefs(soup)\n",
    "\n",
    "        print('getting sub-urls')\n",
    "        sub_urls = [tag.get('href') for tag in reflist if '/sportaanbieders' in tag.get('href', '')]\n",
    "        sub_urls = sub_urls[3:]\n",
    "\n",
    "        print('extracting the data')\n",
    "        print(f'{len(sub_urls)} sub-urls')\n",
    "\n",
    "        # Use the SubUrlIterator here\n",
    "        for sub in self.SubUrlIterator(sub_urls):\n",
    "            try:\n",
    "                sub = self.extract(sub)\n",
    "                site = self.base_url.rstrip('/') + sub\n",
    "                print(f\"Fetching site: {site}\")\n",
    "                soup = self.open_url(site)\n",
    "                if soup is None:\n",
    "                    print(f\"Failed to retrieve the site: {site}. Skipping.\")\n",
    "                    continue\n",
    "                info = self.fetch_sidebar(soup)\n",
    "                if info is None:\n",
    "                    print(f\"No sidebar found for site: {site}. Skipping.\")\n",
    "                    continue\n",
    "                info = self.read_li(info)\n",
    "                phone = self.get_phone(info)\n",
    "                phone = self.remove_html_tags(phone).strip()\n",
    "                email = self.get_email(info)\n",
    "                email = self.remove_html_tags(email).replace(\"/\", \"\")\n",
    "                print(f'{site} ; {phone} ; {email}')\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing {sub}: {e}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to create a Crawler instance and start crawling.\"\"\"\n",
    "    base_url = \"https://sport050.nl/sportaanbieders/alle-aanbieders/\"\n",
    "    crawler = Crawler(base_url)\n",
    "    crawler.crawl_site()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# #Conversion to Iterator Pattern:\n",
    "\n",
    "# The Crawler class now implements the iterator pattern by defining __iter__ and __next__ methods.\n",
    "# __iter__ Method: Returns the iterator object itself. This allows the class to be used in iteration contexts (e.g., in a for loop).\n",
    "# __next__ Method: Handles the logic for fetching and processing URLs. It retrieves the next URL to process, fetches its content, extracts and filters links, and adds new links to the list of sites to crawl. If there are no more sites to crawl, it raises StopIteration.\n",
    "# Loop Transformation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59bae98",
   "metadata": {},
   "source": [
    "# Step 3: Make use of a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbce56b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetch urls\n",
      "HTTP Error: 404 - Not Found\n",
      "Failed to retrieve the URL. Exiting.\n"
     ]
    }
   ],
   "source": [
    "# bellow is  a summary of the refactoring process for the Crawler class to incorporate an iterator and generator:\n",
    "\n",
    "# Generator Implementation: Added the __iter__() method to make Crawler iterable. This method creates a generator that yields\n",
    "# #tuples of crawled data (site URL, phone number, and email).\n",
    "import urllib.request\n",
    "import ssl\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "\n",
    "    def hack_ssl(self):\n",
    "        \"\"\"Ignores the certificate errors\"\"\"\n",
    "        ctx = ssl.create_default_context()\n",
    "        ctx.check_hostname = False\n",
    "        ctx.verify_mode = ssl.CERT_NONE\n",
    "        return ctx\n",
    "\n",
    "    def open_url(self, url):\n",
    "        \"\"\"Reads URL file as a big string and cleans the HTML file to make it more readable. Input: URL, output: soup object\"\"\"\n",
    "        ctx = self.hack_ssl()\n",
    "        try:\n",
    "            response = urllib.request.urlopen(url, context=ctx)\n",
    "            html = response.read()\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            return soup\n",
    "        except urllib.error.HTTPError as e:\n",
    "            print(f\"HTTP Error: {e.code} - {e.reason}\")\n",
    "            return None\n",
    "        except urllib.error.URLError as e:\n",
    "            print(f\"URL Error: {e.reason}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "    def read_hrefs(self, soup):\n",
    "        \"\"\"Get from soup object a list of anchor tags, get the href keys. Input: soup object\"\"\"\n",
    "        return [tag for tag in soup('a')]\n",
    "\n",
    "    def read_li(self, soup):\n",
    "        \"\"\"Get from soup object a list of list items. Input: soup object\"\"\"\n",
    "        return [tag for tag in soup('li')]\n",
    "\n",
    "    def get_phone(self, info):\n",
    "        \"\"\"Extracts phone number from the list of information. Input: list of BeautifulSoup objects\"\"\"\n",
    "        reg = r\"(?:(?:00|\\+)?[0-9]{4})?(?:[ .-][0-9]{3}){1,5}\"\n",
    "        phone = [s for s in info if 'Telefoon' in str(s)]\n",
    "        if phone:\n",
    "            phone = str(phone[0])\n",
    "        else:\n",
    "            phone = [s for s in info if re.findall(reg, str(s))]\n",
    "            if phone:\n",
    "                phone = str(phone[0])\n",
    "            else:\n",
    "                phone = \"\"\n",
    "        return phone.replace('Facebook', '').replace('Telefoon:', '')\n",
    "\n",
    "    def get_email(self, soup):\n",
    "        \"\"\"Extracts email from the soup object. Input: soup object\"\"\"\n",
    "        try:\n",
    "            email = [s for s in soup if '@' in str(s)]\n",
    "            if email:\n",
    "                email = str(email[0])[4:-5]\n",
    "                bs = BeautifulSoup(email, features=\"html.parser\")\n",
    "                email = bs.find('a').attrs['href'].replace('mailto:', '')\n",
    "            else:\n",
    "                email = \"\"\n",
    "        except:\n",
    "            email = \"\"\n",
    "        return email\n",
    "\n",
    "    def remove_html_tags(self, text):\n",
    "        \"\"\"Remove HTML tags from a string\"\"\"\n",
    "        clean = re.compile('<.*?>')\n",
    "        return re.sub(clean, '', text)\n",
    "\n",
    "    def fetch_sidebar(self, soup):\n",
    "        \"\"\"Reads HTML file and extracts sidebar. Input: HTML, output: sidebar\"\"\"\n",
    "        sidebar = soup.findAll(attrs={'class': 'sidebar'})\n",
    "        if sidebar:\n",
    "            return sidebar[0]\n",
    "        return None\n",
    "\n",
    "    def extract(self, url):\n",
    "        \"\"\"Extracts and formats the URL part needed\"\"\"\n",
    "        text = str(url)\n",
    "        return text.split('\"')[0] + \"/\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Generator to yield crawled websites\"\"\"\n",
    "        print('fetch urls')\n",
    "        soup = self.open_url(self.base_url)\n",
    "        if soup is None:\n",
    "            print(\"Failed to retrieve the URL. Exiting.\")\n",
    "            return\n",
    "        \n",
    "        reflist = self.read_hrefs(soup)\n",
    "\n",
    "        print('getting sub-urls')\n",
    "        sub_urls = [tag.get('href') for tag in reflist if '/sportaanbieders' in tag.get('href', '')]\n",
    "        sub_urls = sub_urls[3:]\n",
    "\n",
    "        print('extracting the data')\n",
    "        print(f'{len(sub_urls)} sub-urls')\n",
    "\n",
    "        for sub in sub_urls:\n",
    "            try:\n",
    "                sub = self.extract(sub)\n",
    "                site = self.base_url.rstrip('/') + sub\n",
    "                print(f\"Fetching site: {site}\")\n",
    "                soup = self.open_url(site)\n",
    "                if soup is None:\n",
    "                    print(f\"Failed to retrieve the site: {site}. Skipping.\")\n",
    "                    continue\n",
    "                info = self.fetch_sidebar(soup)\n",
    "                if info is None:\n",
    "                    print(f\"No sidebar found for site: {site}. Skipping.\")\n",
    "                    continue\n",
    "                info = self.read_li(info)\n",
    "                phone = self.get_phone(info)\n",
    "                phone = self.remove_html_tags(phone).strip()\n",
    "                email = self.get_email(info)\n",
    "                email = self.remove_html_tags(email).replace(\"/\", \"\")\n",
    "                yield (site, phone, email)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing {sub}: {e}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to create a Crawler instance and use the generator.\"\"\"\n",
    "    base_url = \"https://sport050.nl/sportaanbieders/alle-aanbieders/\"\n",
    "    crawler = Crawler(base_url)\n",
    "\n",
    "    # Use the generator to get a few results\n",
    "    for site, phone, email in crawler:\n",
    "        print(f'{site} ; {phone} ; {email}')\n",
    "        # Optional: break after a few results for testing\n",
    "        # break  # Uncomment this line to limit output during testing\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    \n",
    "#This approach leverages Python's generator capabilities to efficiently handle and \n",
    "#iterate over a potentially large set of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c02691",
   "metadata": {},
   "source": [
    "# Step  4: Come up with enhancements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8d27c7",
   "metadata": {},
   "source": [
    "1.Single Responsibility Principle (SRP)\n",
    "Current Issue:\n",
    "\n",
    "The Crawler class is handling multiple responsibilities:managing URLs, fetching and processing HTML, andfiltering links.\n",
    "Enhancement:\n",
    "\n",
    "Separation of Concerns:extract different responsibilities into separate classes.For instance:\n",
    "Fetcher:handles the fetching of HTML content.\n",
    "LinkExtractor:extracts and filters links from the HTML.\n",
    "Crawler:manages the overall crawling process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5d9f14",
   "metadata": {},
   "source": [
    "2.Open/Closed Principle (OCP)\n",
    "Current Issue:\n",
    "\n",
    "Adding new features or changing behavior might require modifying the Crawler class itself.\n",
    "\n",
    "Enhancement:\n",
    "\n",
    "Extend Behavior:use composition to allow extending the behavior of the Fetcher and LinkExtractor without modifying them.For instance,we could subclass Fetcher to implement different fetching strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85de9d1c",
   "metadata": {},
   "source": [
    "3.Liskov Substitution Principle (LSP)\n",
    "Current Issue:\n",
    "\n",
    "If subclasses are added (like CustomFetcher),they must be replaceable for the Fetcher class without altering the expected behavior.\n",
    "\n",
    "Enhancement:\n",
    "\n",
    "Ensuring that any subclass of Fetcher or LinkExtractor maintains the expected interface and behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186dcba8",
   "metadata": {},
   "source": [
    "5.dependency Inversion Principle (DIP)\n",
    "Current Issue:\n",
    "\n",
    "The Crawler class is directly dependent on concrete implementations of Fetcher and LinkExtractor.\n",
    "\n",
    "enhancement:\n",
    "\n",
    "invert Dependencies: Depend on abstractions (interfaces) rather than concrete implementations.This way, we can inject different implementations without changing the Crawler."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
